{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We trust you have received the usual lecture from the local System\n",
      "Administrator. It usually boils down to these three things:\n",
      "\n",
      "    #1) Respect the privacy of others.\n",
      "    #2) Think before you type.\n",
      "    #3) With great power comes great responsibility.\n",
      "\n",
      "[sudo] password for alice: \n"
     ]
    }
   ],
   "source": [
    "!sudo hdfs dfs -mkdir -p /user/hadoop-repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -put text hadoop-repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config fsck hadoop-repo/text/complete-shakespeare.txt -files -blocks -locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat hadoop-repo/text/complete-shakespeare.txt \\\n",
    "    2>/dev/null | head -n 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/wordcountMapper.py\n",
    "\n",
    "#!/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/bin/python   \n",
    "\n",
    "import sys                                                                                                \n",
    "for oneLine in sys.stdin:\n",
    "    oneLine = oneLine.strip()\n",
    "    for word in oneLine.split(\" \"):\n",
    "        if word != \"\":\n",
    "            print ('%s\\t%s' % (word, 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -cat hadoop-repo/text/complete-shakespeare.txt \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 20 \\\n",
    "    | python ./codes/wordcountMapper.py\n",
    "    | sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/wordcountReducer.py\n",
    "#!/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "total_word_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split(\"\\t\", 1)\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if current_word == word:\n",
    "        total_word_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print (\"%s\\t%s\" % (current_word, total_word_count))\n",
    "        current_word = word\n",
    "        total_word_count = 1\n",
    "        \n",
    "if current_word == word:\n",
    "    print (\"%s\\t%s\" % (current_word, total_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -cat hadoop-repo/text/complete-shakespeare.txt \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 20 \\\n",
    "    | python ./codes/wordcountMapper.py\n",
    "    | sort \\\n",
    "        | python ./codes/wordcountReducer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -rm -R hadoop-repo/output-wordcount\n",
    "\n",
    "!mapred --config ~/hadoop_palmetto/config streaming \\\n",
    "    -input intro-to-hadoop/text/complete-shakespeare.txt \\\n",
    "    -output hadoop-repo/output-wordcount \\\n",
    "    -file ./codes/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file ./codes/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hadoop-repo/output-wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat hadoop-repo/output-wordcount/part-00000 \\\n",
    "    2>/dev/null | head -n 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/wordcountEnhancedMapper.py\n",
    "#!/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/bin/python                                          \n",
    "import sys                     \n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "for oneLine in sys.stdin:\n",
    "    oneLine = oneLine.strip()\n",
    "    for word in oneLine.split(\" \"):\n",
    "        if word != \"\":\n",
    "            newWord = word.translate(translator).lower()\n",
    "            print ('%s\\t%s' % (_______, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -R hadoop-repo/output-wordcount-enhanced\n",
    "\n",
    "!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input hadoop-repo/text/gutenberg-shakespeare.txt \\\n",
    "    -output hadoop-repo/output-wordcount \\\n",
    "    -file ____________________________________________________ \\\n",
    "    -mapper _____________________ \\\n",
    "    -file ____________________________________________________ \\\n",
    "    -reducer _____________________ \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Debug jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset\n",
    "http://files.grouplens.org/datasets/movielens/ml-10m-README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -h /repository/movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find movies which have the highest average ratings over the years and report their ratings and genres\n",
    "Find the average ratings of all movies over the years\n",
    "Sort the average ratings from highest to lowest\n",
    "Report the results, augmented by genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /repository/movielens\n",
    "!hdfs dfs -cat /repository/movielens/README.txt\n",
    "\n",
    "!hdfs dfs -cat /repository/movielens/links.csv \\\n",
    "    2>/dev/null | head -n 5\n",
    "\n",
    "!hdfs dfs -cat /repository/movielens/movies.csv \\\n",
    "    2>/dev/null | head -n 5\n",
    "\n",
    "!hdfs dfs -cat /repository/movielens/ratings.csv \\\n",
    "    2>/dev/null | head -n 5\n",
    "\n",
    "!hdfs dfs -cat /repository/movielens/tags.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgRatingMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv \n",
    "\n",
    "movieFile = \"./movielens/movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        movieTitle = movieList[ratingInfo[1]][\"title\"]\n",
    "        movieGenre = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        print (\"%s\\t%s\\t%s\" % (movieTitle, rating, movieGenre))\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv \\\n",
    "    2>/dev/null | head -n 10 | python ./codes/avgRatingMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir movielens\n",
    "!mkdir movielens\n",
    "!hdfs dfs -get /repository/movielens/movies.csv movielens/movies.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Reducer \n",
    "%%writefile codes/avgRatingReducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "current_movie = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    movie, rating, genre = line.split(\"\\t\", 2)\n",
    "    try:\n",
    "        rating = float(rating)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if current_movie == movie:\n",
    "        current_rating_sum += rating\n",
    "        current_rating_count += 1\n",
    "    else:\n",
    "        if current_movie:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            print (\"%s\\t%s\\t%s\" % (current_movie, rating_average, genre))    \n",
    "        current_movie = movie\n",
    "        current_rating_sum = rating\n",
    "        current_rating_count = 1\n",
    "\n",
    "if current_movie == movie:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    print (\"%s\\t%s\\t%s\" % (current_movie, rating_average, genre))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 5 \\\n",
    "    | python ./codes/avgRatingMapper.py \\\n",
    "    | sort \\\n",
    "    | python ./codes/avgRatingReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS execution\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output hadoop-repo/output-movielens-01 \\\n",
    "    -file ./codes/avgRatingMapper.py \\\n",
    "    -mapper avgRatingMapper.py \\\n",
    "    -file ./codes/avgRatingReducer.py \\\n",
    "    -reducer avgRatingReducer.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log YARN \n",
    "! yarn logs -applicationId APPLICATION_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!yarn logs -applicationId application_1476193845089_0123 (tail -l 500)\n",
    "stderr: Error messages from the actual task execution\n",
    "stdout: Print out messages if the task includes them\n",
    "syslog: Logging messages from the Hadoop MapReduce operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yarn logs -applicationId application_1505269880969_0056 | grep -v INFO\n",
    "\n",
    "!yarn logs -applicationId APPLICATION_ID | grep '^Container:'\n",
    "\n",
    "!yarn logs -applicationId application_1505269880969_0056 | grep '^Container:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yarn logs -applicationId APPLICATION_ID -containerId CONTAINER_ID --nodeAddress NODE_ADDRESS \\\n",
    "    | grep -v DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AvgMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgRatingMapper02.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        movieTitle = movieList[ratingInfo[1]][\"title\"]\n",
    "        movieGenre = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        if _________:\n",
    "            print (\"%s\\t%s\\t%s\" % (movieTitle, rating, movieGenre))\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output hadoop-repo/output-movielens-challenge \\\n",
    "    -file ____________ \\\n",
    "    -mapper ___________ \\\n",
    "    -file ./codes/avgRatingReducer01.py \\\n",
    "    -reducer avgRatingReducer01.py \\\n",
    "    -file ./codes/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data movement in the shuffle phase\n",
    "\n",
    "!hdfs dfs -rm -r hadoop-repo/output-movielens-02\n",
    "\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output hadoop-repo/output-movielens-02 \\\n",
    "    -file ./codes/avgRatingMapper.py \\\n",
    "    -mapper avgRatingMapper.py \\\n",
    "    -file ./codes/avgRatingReducer.py \\\n",
    "    -reducer avgRatingReducer.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgRatingReducer-exp.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "current_movie = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    movie, rating = line.split(\"\\t\", 1)\n",
    "    try:\n",
    "        rating = float(rating)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if current_movie == movie:\n",
    "        current_rating_sum += rating\n",
    "        current_rating_count += 1\n",
    "    else:\n",
    "        if current_movie:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            movieTitle = movieList[current_movie][\"title\"]\n",
    "            movieGenres = movieList[current_movie][\"genre\"]\n",
    "            print (\"%s\\t%s\\t%s\" % (movieTitle, rating_average, movieGenres))    \n",
    "        current_movie = movie\n",
    "        current_rating_sum = rating\n",
    "        current_rating_count = 1\n",
    "\n",
    "if current_movie == movie:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    movieTitle = movieList[current_movie][\"title\"]\n",
    "    movieGenres = movieList[current_movie][\"genre\"]\n",
    "    print (\"%s\\t%s\\t%s\" % (movieTitle, rating_average, movieGenres))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r hadoop-repo/output-movielens-03\n",
    "\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output hadoop-repo/output-movielens-03 \\\n",
    "    -file ./codes/avgRatingMapper-exp.py \\\n",
    "    -mapper avgRatingMapper-exp.py \\\n",
    "    -file ./codes/avgRatingReducer.py \\\n",
    "    -reducer avgRatingReducer.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output dir\n",
    "!hdfs dfs -ls hadoop-repo/output-movielens-02\n",
    "!hdfs dfs -ls hadoop-repo/output-movielens-03\n",
    "\n",
    "!hdfs dfs -cat hadoop-repo/output-movielens-03/part-00000 \\\n",
    "    2>/dev/null | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find genres which have the highest average ratings over the years\n",
    "# Common optimization approaches:\n",
    "\n",
    "# In-mapper reduction of key/value pairs\n",
    "# Additional combiner function\n",
    "%%writefile codes/avgGenreMapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# for nonHDFS run\n",
    "movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "#movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genreList = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genreList.split(\"|\"):\n",
    "            print (\"%s\\t%s\" % (genre, rating))\n",
    "    except ValueError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization through in-mapper reduction of Key/Value pairs\n",
    "%%writefile codes/avgGenreMapper-comb.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# for nonHDFS run\n",
    "movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "#movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genreList = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genreList.split(\"|\"):\n",
    "            print (\"%s\\t%s\" % (genre, rating))\n",
    "    except ValueError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "%%writefile codes/avgGenreReducer-comb.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "current_genre = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, rating = line.split(\"\\t\", 1)\n",
    "\n",
    "    if current_genre == genre:\n",
    "        try:\n",
    "            current_rating_sum += float(rating)\n",
    "            current_rating_count += 1\n",
    "        except ValueError:\n",
    "            continue    \n",
    "    else:\n",
    "        if current_genre:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            print (\"%s\\t%s\" % (current_genre, rating_average))    \n",
    "        current_genre = genre\n",
    "        try:\n",
    "            current_rating_sum = float(rating)\n",
    "            current_rating_count = 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "if current_genre == genre:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    print (\"%s\\t%s\" % (current_genre, rating_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -cat hadoop-repo/output-movielens-04/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python ./codes/avgGenreMapper-comb.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "#     | head -n 10 \\\n",
    "#     | python ./codes/avgGenreMapper02.py \\\n",
    "#     | sort \\\n",
    "#     | python ./codes/avgGenreReducer02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreCombiner.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "genreList = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "\n",
    "    if genre in genreList:\n",
    "        genreList[genre][\"total_rating\"] += ratingInfo[\"total_rating\"]\n",
    "        genreList[genre][\"total_count\"] += ratingInfo[\"total_count\"]\n",
    "    else:\n",
    "        genreList[genre] = {}\n",
    "        genreList[genre][\"total_rating\"] = ratingInfo[\"total_rating\"]\n",
    "        genreList[genre][\"total_count\"] = 1\n",
    "\n",
    "for genre in genreList:\n",
    "    print (\"%s\\t%s\" % (genre, json.dumps(genreList[genre])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output hadoop-repo/output-movielens-06 \\\n",
    "    -file ./codes/avgGenreMapper02.py \\\n",
    "    -mapper avgGenreMapper02.py \\\n",
    "    -file ./codes/avgGenreReducer02.py \\\n",
    "    -reducer avgGenreReducer02.py \\\n",
    "    -file ./codes/avgGenreCombiner.py \\\n",
    "    -combiner avgGenreCombiner.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Integrating Hadoop job into workflow HDP workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile movieAnalyzer.pbs\n",
    "#!/bin/bash\n",
    "\n",
    "#PBS -N movieData\n",
    "#PBS -l select=1:ncpus=8:mem=8gb\n",
    "#PBS -l walltime=00:15:00\n",
    "#PBS -j oe\n",
    "\n",
    "# load hdp module and initilalize Keberos tokens\n",
    "module load hdp/0.1\n",
    "cypress-kinit\n",
    "klist\n",
    "\n",
    "# cd into directory containing the PBS script\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# attempt to remove output directory\n",
    "hdfs dfs -rm -r hadoop-repo/output-movielens-03\n",
    "\n",
    "# \n",
    "# submit Hadoop job to Cypress\n",
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output hadoop-repo/output-movielens-03 \\\n",
    "    -file ./codes/avgRatingMapper01.py \\\n",
    "    -mapper avgRatingMapper01.py \\\n",
    "    -file ./codes/avgRatingReducer.py \\\n",
    "    -reducer avgRatingReducer.py \\\n",
    "    -file ./movielens/movies.csv\n",
    "\n",
    "# export output data back to Palmetto for further analysis\n",
    "hdfs dfs -get hadoop-repo/output-movielens-03/part-00000 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View final output when job is finished\n",
    "\n",
    "ssh login001\n",
    "cd ~/hadoop-repo\n",
    "qsub movieAnalyzer.pbs\n",
    "\n",
    "\n",
    "!qstat -anu $USER\n",
    "!cat part-00000 2>/dev/null | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLean up \n",
    "!hdfs dfs -ls hadoop-repo\n",
    "!hdfs dfs -rm -r hadoop-repo/\n",
    "!rm -Rf codes/\n",
    "!rm -Rf movielens/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
