{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- DataFrame API docs: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "Rules of thumb:\n",
    "- Hit tab to auto-complete\n",
    "- To see all available methods, place a dot (.) after the RDD (e.g. words.) and hit tab \n",
    "- Use `.collect()` to see the contents of the RDD\n",
    "\n",
    "Solutions for potentially challenging exercises can be found in the end of the section. Don't peek unless you're really stuck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.171.127.48:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1122dff28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# like in the pyspark shell, SparkSession is already defined\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DataFrame methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"../data/people/names.json\")\n",
    "\n",
    "# other supported file formats:\n",
    "# spark.read.parquet(\"../data/pems_sorted/\")\n",
    "# spark.read.text()\n",
    "# spark.read.csv()\n",
    "# spark.read.orc()\n",
    "\n",
    "# generic form: \n",
    "# spark.read.load(\"path/to/someFile.csv\", format=\"csv\", sep=\":\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# Loading data from a JDBC source\n",
    "# jdbcDF = spark.read \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "#     .option(\"dbtable\", \"schema.tablename\") \\\n",
    "#     .option(\"user\", \"username\") \\\n",
    "#     .option(\"password\", \"password\") \\\n",
    "#     .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write reading different files in ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timeperiod: string, flow1: int, occupancy1: double, speed1: double, flow2: int, occupancy2: double, speed2: double, flow3: int, occupancy3: double, speed3: double, flow4: int, occupancy4: double, speed4: double, flow5: int, occupancy5: double, speed5: double, flow6: int, occupancy6: double, speed6: double, flow7: int, occupancy7: double, speed7: double, flow8: int, occupancy8: double, speed8: double, station: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"../data/pems_sorted/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+--------------+----------+-----------+------------+-------------+---------+--------------------+\n",
      "|ClientID|QueryTime|Market|DevicePlatform|DeviceMake|DeviceModel|       State|      Country|SessionId|SessionPageViewOrder|\n",
      "+--------+---------+------+--------------+----------+-----------+------------+-------------+---------+--------------------+\n",
      "|       8| 18:54:20| en-US|       Android|   Samsung|   SCH-i500|  California|United States|        0|                   0|\n",
      "|      23| 19:19:44| en-US|       Android|       HTC| Incredible|Pennsylvania|United States|        0|                   0|\n",
      "|      23| 19:19:46| en-US|       Android|       HTC| Incredible|Pennsylvania|United States|        0|                   1|\n",
      "+--------+---------+------+--------------+----------+-----------+------------+-------------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"../data/mobile_data/MobileSampleData.csv\", header=True).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data output (writing to disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- API docs: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to a file\n",
    "df.write.parquet(\"new_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite on save\n",
    "df.write.mode(\"overwrite\").parquet(\"new_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can read from any format and write to any format (barring formatting limitations/rules):\n",
    "# df.write.csv(\"new_data.csv\",header=True)\n",
    "# df.write.json(\"new_data.json\")\n",
    "# df.write.orc(\"new_data.orc\")\n",
    "# df.write.parquet(\"new_data.parquet\")\n",
    "\n",
    "# generic form:\n",
    "# df.write.save(\"fileName.parquet\", format=\"parquet\")\n",
    "# df.write.mode(\"overwrite\").save(\"fileName.parquet\", format=\"parquet\")\n",
    "\n",
    "# Saving data to a JDBC source\n",
    "# jdbcDF.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "#     .option(\"dbtable\", \"schema.tablename\") \\\n",
    "#     .option(\"user\", \"username\") \\\n",
    "#     .option(\"password\", \"password\") \\\n",
    "#     .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"../data/people/names.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AGE=36, gender='female', height=180, name='Zoe'),\n",
       " Row(AGE=23, gender='female', height=165, name='Alice'),\n",
       " Row(AGE=30, gender='male', height=175, name='Andy'),\n",
       " Row(AGE=25, gender='female', height=170, name='Jane'),\n",
       " Row(AGE=None, gender='male', height=165, name='Michael')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "| AGE|gender|height|   name|\n",
      "+----+------+------+-------+\n",
      "|  36|female|   180|    Zoe|\n",
      "|  23|female|   165|  Alice|\n",
      "|  30|  male|   175|   Andy|\n",
      "|  25|female|   170|   Jane|\n",
      "|null|  male|   165|Michael|\n",
      "+----+------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AGE=36, gender='female', height=180, name='Zoe'),\n",
       " Row(AGE=23, gender='female', height=165, name='Alice'),\n",
       " Row(AGE=30, gender='male', height=175, name='Andy'),\n",
       " Row(AGE=25, gender='female', height=170, name='Jane'),\n",
       " Row(AGE=None, gender='male', height=165, name='Michael')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n",
      "|AGE|gender|height| name|\n",
      "+---+------+------+-----+\n",
      "| 36|female|   180|  Zoe|\n",
      "| 23|female|   165|Alice|\n",
      "| 30|  male|   175| Andy|\n",
      "+---+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit(n) returns a new dataframe with the first n rows of the dataframe\n",
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "| AGE|gender|height|   name|\n",
      "+----+------+------+-------+\n",
      "|  36|female|   180|    Zoe|\n",
      "|  23|female|   165|  Alice|\n",
      "|  30|  male|   175|   Andy|\n",
      "|  25|female|   170|   Jane|\n",
      "|null|  male|   165|Michael|\n",
      "|  19|  male|   180| Justin|\n",
      "+----+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AGE: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGE', 'gender', 'height', 'name']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+-----------------+-----+\n",
      "|summary|              AGE|gender|           height| name|\n",
      "+-------+-----------------+------+-----------------+-----+\n",
      "|  count|                5|     6|                6|    6|\n",
      "|   mean|             26.6|  null|            172.5| null|\n",
      "| stddev|6.580273550544841|  null|6.892024376045112| null|\n",
      "|    min|               19|female|              165|Alice|\n",
      "|    max|               36|  male|              180|  Zoe|\n",
      "+-------+-----------------+------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting specific columns in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'name'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting a column\n",
    "# Column API docs: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column\n",
    "df['name']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a new dataframe with only selected columns\n",
    "df.select('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|    Zoe|  36|\n",
      "|  Alice|  23|\n",
      "|   Andy|  30|\n",
      "|   Jane|  25|\n",
      "|Michael|null|\n",
      "| Justin|  19|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a new dataframe with only selected columns\n",
    "df.select(['name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "df = df.withColumnRenamed('AGE', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, gender: string, height: bigint, name: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+---------------+\n",
      "| age|gender|height|   name|height plus 100|\n",
      "+----+------+------+-------+---------------+\n",
      "|  36|female|   180|    Zoe|            280|\n",
      "|  23|female|   165|  Alice|            265|\n",
      "|  30|  male|   175|   Andy|            275|\n",
      "|  25|female|   170|   Jane|            270|\n",
      "|null|  male|   165|Michael|            265|\n",
      "|  19|  male|   180| Justin|            280|\n",
      "+----+------+------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating new columns\n",
    "df = df.withColumn('height plus 100', df.height + 100)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+---------------+-------+\n",
      "| age|gender|height|   name|height plus 100|is_tall|\n",
      "+----+------+------+-------+---------------+-------+\n",
      "|  36|female|   180|    Zoe|            280|   true|\n",
      "|  23|female|   165|  Alice|            265|  false|\n",
      "|  30|  male|   175|   Andy|            275|   true|\n",
      "|  25|female|   170|   Jane|            270|  false|\n",
      "|null|  male|   165|Michael|            265|  false|\n",
      "|  19|  male|   180| Justin|            280|   true|\n",
      "+----+------+------+-------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating new columns\n",
    "df = df.withColumn('is_tall', df.height >= 175)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "`.filter()` takes in either (i) a `Column` of `types.BooleanType` or (ii) a string of SQL expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+---------------+-------+\n",
      "|age|gender|height|name|height plus 100|is_tall|\n",
      "+---+------+------+----+---------------+-------+\n",
      "| 36|female|   180| Zoe|            280|   true|\n",
      "| 30|  male|   175|Andy|            275|   true|\n",
      "| 25|female|   170|Jane|            270|  false|\n",
      "+---+------+------+----+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter using SQL expressions\n",
    "# df.where('age >= 25').show() is also possible because .where() is an alias for .filter()\n",
    "df.filter('age >= 25').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+---------------+-------+\n",
      "|age|gender|height|name|height plus 100|is_tall|\n",
      "+---+------+------+----+---------------+-------+\n",
      "| 36|female|   180| Zoe|            280|   true|\n",
      "| 30|  male|   175|Andy|            275|   true|\n",
      "| 25|female|   170|Jane|            270|  false|\n",
      "+---+------+------+----+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter using a column of boolean types\n",
    "df.filter(df.age >= 25).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(age >= 25)'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.age >= 25 returns a Column of booleans\n",
    "df.age >= 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+---------------+-------+\n",
      "|age|gender|height|name|height plus 100|is_tall|\n",
      "+---+------+------+----+---------------+-------+\n",
      "| 30|  male|   175|Andy|            275|   true|\n",
      "| 25|female|   170|Jane|            270|  false|\n",
      "+---+------+------+----+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter( (df.age >= 25) & (df.age <= 30) ).show()\n",
    "# you can use df.age or df['age']\n",
    "# you can replace & with | for 'or' operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy\n",
    "\n",
    "TL;DR - `.groupBy()` allows you to group rows together based on its value in some given column(s)\n",
    "- `df.groupBy([cols])`\n",
    "- [GroupedData operations](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) (alternatively, you can instantiate a variable with the type of GroupedData, let jupyter notebook's intellisense show you what methods are available:\n",
    "    - `grouped = df.groupBy('gender')`\n",
    "    - `grouped.` (and hit tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=36, gender='female', height=180, name='Zoe', height plus 100=280, is_tall=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1123f3978>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupBy('gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|female|    3|\n",
      "|  male|    3|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+--------------------+\n",
      "|gender|avg(age)|       avg(height)|avg(height plus 100)|\n",
      "+------+--------+------------------+--------------------+\n",
      "|female|    28.0|171.66666666666666|   271.6666666666667|\n",
      "|  male|    24.5|173.33333333333334|   273.3333333333333|\n",
      "+------+--------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('gender').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+\n",
      "|sum(age)|sum(height)|sum(height plus 100)|\n",
      "+--------+-----------+--------------------+\n",
      "|     133|       1035|                1635|\n",
      "+--------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate total age and height\n",
    "df.groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate average height of all people (i.e. don't groupby anything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate max height for each gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate min height for each gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Crimes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes = spark.read.csv(\"../data/crimes/Crimes_-_One_year_prior_to_present.csv\", header=True, inferSchema=True)\n",
    "# try the above without the header and inferSchema option. see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CASE#: string (nullable = true)\n",
      " |-- DATE  OF OCCURRENCE: string (nullable = true)\n",
      " |-- BLOCK: string (nullable = true)\n",
      " |--  IUCR: string (nullable = true)\n",
      " |--  PRIMARY DESCRIPTION: string (nullable = true)\n",
      " |--  SECONDARY DESCRIPTION: string (nullable = true)\n",
      " |--  LOCATION DESCRIPTION: string (nullable = true)\n",
      " |-- ARREST: string (nullable = true)\n",
      " |-- DOMESTIC: string (nullable = true)\n",
      " |-- BEAT: integer (nullable = true)\n",
      " |-- WARD: integer (nullable = true)\n",
      " |-- FBI CD: string (nullable = true)\n",
      " |-- X COORDINATE: integer (nullable = true)\n",
      " |-- Y COORDINATE: integer (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: print the schema of the dataframe (e.g. data type of each column)?\n",
    "crimes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263191"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - how many rows are there in the dataframe?\n",
    "crimes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CASE#='JB241987', DATE  OF OCCURRENCE='04/28/2018 10:05:00 PM', BLOCK='009XX N LONG AVE',  IUCR='2092',  PRIMARY DESCRIPTION='NARCOTICS',  SECONDARY DESCRIPTION='SOLICIT NARCOTICS ON PUBLICWAY',  LOCATION DESCRIPTION='SIDEWALK', ARREST='Y', DOMESTIC='N', BEAT=1524, WARD=37, FBI CD='18', X COORDINATE=1140136, Y COORDINATE=1905903, LATITUDE=41.897894893, LONGITUDE=-87.760743714, LOCATION='(41.897894893, -87.760743714)'),\n",
       " Row(CASE#='JA430240', DATE  OF OCCURRENCE='09/06/2017 01:30:00 PM', BLOCK='032XX W 26TH ST',  IUCR='0810',  PRIMARY DESCRIPTION='THEFT',  SECONDARY DESCRIPTION='OVER $500',  LOCATION DESCRIPTION='OTHER', ARREST='Y', DOMESTIC='N', BEAT=1024, WARD=12, FBI CD='06', X COORDINATE=1155313, Y COORDINATE=1886555, LATITUDE=41.844510467, LONGITUDE=-87.705519454, LOCATION='(41.844510467, -87.705519454)')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Display the first 2 rows\n",
    "crimes.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CASE#',\n",
       " 'DATE  OF OCCURRENCE',\n",
       " 'BLOCK',\n",
       " ' IUCR',\n",
       " ' PRIMARY DESCRIPTION',\n",
       " ' SECONDARY DESCRIPTION',\n",
       " ' LOCATION DESCRIPTION',\n",
       " 'ARREST',\n",
       " 'DOMESTIC',\n",
       " 'BEAT',\n",
       " 'WARD',\n",
       " 'FBI CD',\n",
       " 'X COORDINATE',\n",
       " 'Y COORDINATE',\n",
       " 'LATITUDE',\n",
       " 'LONGITUDE',\n",
       " 'LOCATION']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: What columns are in the dataframe?\n",
    "crimes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CASE#',\n",
       " 'DATE  OF OCCURRENCE',\n",
       " 'BLOCK',\n",
       " 'IUCR',\n",
       " 'PRIMARY DESCRIPTION',\n",
       " 'SECONDARY DESCRIPTION',\n",
       " 'LOCATION DESCRIPTION',\n",
       " 'ARREST',\n",
       " 'DOMESTIC',\n",
       " 'BEAT',\n",
       " 'WARD',\n",
       " 'FBI CD',\n",
       " 'X COORDINATE',\n",
       " 'Y COORDINATE',\n",
       " 'LATITUDE',\n",
       " 'LONGITUDE',\n",
       " 'LOCATION']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's rename the improperly formatted column names\n",
    "columnNames = crimes.columns\n",
    "for col in columnNames:\n",
    "    crimes = crimes.withColumnRenamed(col, col.strip())\n",
    "    \n",
    "crimes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|ARREST| count|\n",
      "+------+------+\n",
      "|     Y| 50126|\n",
      "|     N|213065|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: How many cases resulted in arrest, and how many didn’t?\n",
    "# Hint: Highlight whitespace between this cell and the next cell to see the hint\n",
    "\n",
    "crimes.groupBy(\"ARREST\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">Use .groupBy(\"ARREST\")</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|WARD|count|\n",
      "+----+-----+\n",
      "|  31| 3258|\n",
      "|  34| 6845|\n",
      "|  28|11071|\n",
      "|  27|10000|\n",
      "|  26| 3498|\n",
      "|  44| 3968|\n",
      "|  12| 3036|\n",
      "|  22| 3014|\n",
      "|  47| 2708|\n",
      "|null|    1|\n",
      "|   1| 4873|\n",
      "|  13| 3086|\n",
      "|  16| 6234|\n",
      "|   6| 8427|\n",
      "|   3| 6860|\n",
      "|  20| 7608|\n",
      "|  40| 2941|\n",
      "|  48| 2641|\n",
      "|   5| 6252|\n",
      "|  19| 2207|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: List the total count of cases for each WARD\n",
    "crimes.groupBy(\"WARD\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|WARD|count|\n",
      "+----+-----+\n",
      "|null|    1|\n",
      "|  19| 2207|\n",
      "|  36| 2536|\n",
      "|  39| 2598|\n",
      "|  48| 2641|\n",
      "|  33| 2698|\n",
      "|  47| 2708|\n",
      "|  38| 2738|\n",
      "|  45| 2761|\n",
      "|  50| 2877|\n",
      "|  41| 2913|\n",
      "|  40| 2941|\n",
      "|  22| 3014|\n",
      "|  12| 3036|\n",
      "|  13| 3086|\n",
      "|  14| 3146|\n",
      "|  23| 3155|\n",
      "|  35| 3181|\n",
      "|  11| 3216|\n",
      "|  30| 3249|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: List the total count of cases for each WARD, and sort it (by count) in ascending order\n",
    "crimes.groupBy(\"WARD\").count().sort(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|WARD|count|\n",
      "+----+-----+\n",
      "|  42|18548|\n",
      "|  24|12457|\n",
      "|   2|11483|\n",
      "|  28|11071|\n",
      "|  27|10000|\n",
      "|  17| 8595|\n",
      "|   6| 8427|\n",
      "|  21| 8043|\n",
      "|  20| 7608|\n",
      "|   3| 6860|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Show top 10 (WARD, count) pairs with the most number of cases\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "crimes.groupBy(\"WARD\").count().sort(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "| PRIMARY DESCRIPTION|count|\n",
      "+--------------------+-----+\n",
      "|               THEFT|64285|\n",
      "|             BATTERY|49276|\n",
      "|     CRIMINAL DAMAGE|28118|\n",
      "|             ASSAULT|19740|\n",
      "|  DECEPTIVE PRACTICE|17923|\n",
      "|       OTHER OFFENSE|16561|\n",
      "|            BURGLARY|12040|\n",
      "|           NARCOTICS|11664|\n",
      "|             ROBBERY|11080|\n",
      "| MOTOR VEHICLE THEFT|10558|\n",
      "|   CRIMINAL TRESPASS| 6832|\n",
      "|   WEAPONS VIOLATION| 5003|\n",
      "|OFFENSE INVOLVING...| 2247|\n",
      "| CRIM SEXUAL ASSAULT| 1539|\n",
      "|PUBLIC PEACE VIOL...| 1386|\n",
      "+--------------------+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: List top 15 categories (PRIMARY DESCRIPTION) of cases\n",
    "crimes.groupBy('PRIMARY DESCRIPTION').count().sort(desc(\"count\")).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|LOCATION DESCRIPTION|count|\n",
      "+--------------------+-----+\n",
      "|              STREET|58758|\n",
      "|           RESIDENCE|43732|\n",
      "|           APARTMENT|33277|\n",
      "|            SIDEWALK|20542|\n",
      "|               OTHER|10791|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: List top 5 locations (LOCATION DESCRIPTION) where cases occur\n",
    "crimes.groupBy('LOCATION DESCRIPTION').count().sort(desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save one of the results to disk (choose any format)\n",
    "# Note: if your dataframe ends up being partitioned, you can call `your_df.coalesce(1)` before saving (`df.coalesce(1).write...`)\n",
    "top_locations = crimes.groupBy('LOCATION DESCRIPTION').count().sort(desc(\"count\")).limit(5)\n",
    "top_locations.coalesce(1).write.mode(\"overwrite\").csv(\"../data/top_locations\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: submit the preceeding task as a spark job\n",
    "# 1. Create a python file named jobs/top_20_crime_locations.py\n",
    "# 2. define spark session object\n",
    "#   - from pyspark.sql import SparkSession\n",
    "#   - spark = SparkSession.builder.appName(\"MyAppName\").getOrCreate()\n",
    "# 3. Copy the code in the preceeding cell into the file \n",
    "# 4. submit the job: ${SPARK_HOME}/bin/spark-submit --master local ./jobs/top_20_crime_locations.py\n",
    "\n",
    "# if you get stuck, you can refer to ./jobs/top_N_crime_locations_solution.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your creativity - create any other interesting DataFrames or insights into the crimes data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL with Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"../data/people/names.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AGE: bigint, gender: string, height: bigint, name: string]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM names\")\n",
    "# add .show() to see the resulting dataframe. Example:\n",
    "# df = spark.sql(\"SELECT * FROM names\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "|AGE|gender|height|  name|\n",
      "+---+------+------+------+\n",
      "| 36|female|   180|   Zoe|\n",
      "| 30|  male|   175|  Andy|\n",
      "| 19|  male|   180|Justin|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM names WHERE height > 170\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_data_eng_bootcamp",
   "language": "python",
   "name": ".venv_data_eng_bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
